{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9922d831-e320-4baf-876b-4c4dcc905c6f",
   "metadata": {},
   "source": [
    "# Apriori Algorithm\n",
    "1. Finding the frequent 1-itemsets\n",
    "2. Generating candidates for n-itemsets and pruning them\n",
    "3. Calculating the support of the candidates and saving the frequent n-itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be1a406-2d88-44f1-90ed-186d70982f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Apriori:    \n",
    "    def __init__(self, records, relative_min_support):\n",
    "        self.records = records\n",
    "        self.relative_min_support = relative_min_support\n",
    "        self.absolute_min_support = len(records)*relative_min_support\n",
    "        \n",
    "        self._mapping_records(records)\n",
    "        self._get_l1_itemset()\n",
    "        self._get_frequent_itemsets()\n",
    "        \n",
    "    def _mapping_records(self, records):\n",
    "        # collecting unique items\n",
    "        items = list(set(item for record in records for item in record))\n",
    "\n",
    "        # mapping items to ids\n",
    "        self.id_to_item = items\n",
    "        self.item_to_id = {item: i for (i, item) in enumerate(items)}\n",
    "\n",
    "        # mapping items in records to the ids\n",
    "        mapped_records = []\n",
    "        for record in records:\n",
    "            record_items = []\n",
    "            for item in record:\n",
    "                record_items.append(self.item_to_id[item])\n",
    "            mapped_records.append(tuple(sorted(record_items)))\n",
    "\n",
    "        self.mapped_records = mapped_records\n",
    "    \n",
    "    def _get_l1_itemset(self):\n",
    "        records = self.mapped_records\n",
    "        max_id = len(self.id_to_item)-1\n",
    "        absolute_min_support = self.absolute_min_support\n",
    "        \n",
    "        # calculating support of length-1 itemsets\n",
    "        l1_items = {item_id: 0 for item_id in range(0, max_id + 1)}\n",
    "        for record in records:\n",
    "            for item in record:\n",
    "                l1_items[item] = l1_items[item] + 1\n",
    "\n",
    "        # frequent length-1 itemsets\n",
    "        frequent_l1_items = {item: sup for item, sup in l1_items.items() if sup >= absolute_min_support}\n",
    "\n",
    "        self.frequent_items = {(item,): sup for item, sup in frequent_l1_items.items()}\n",
    "        \n",
    "    def _get_frequent_itemsets(self):\n",
    "        # sorting l1 key values (item ids) by ascending order\n",
    "        candidates = [item for item in sorted(self.frequent_items)]\n",
    "\n",
    "        while len(candidates) > 0:\n",
    "            # generate new candidates (l_n itemsets -> l_n+1 itemsets)\n",
    "            new_candidates = self._generate(candidates)\n",
    "            \n",
    "            # calculating the support\n",
    "            # the itemsets need to be a tuple to be hashable for the dictionary\n",
    "            candidate_support = {tuple(itemset): self._calculate_support(itemset) for itemset in new_candidates}\n",
    "            \n",
    "            # considering only the frequent itemsets\n",
    "            frequent_candidates = dict(\n",
    "                filter(lambda itemset_support: itemset_support[1] >= self.absolute_min_support, candidate_support.items()))\n",
    "            \n",
    "            # adding the new itemsets to the dict with frequent itemsets\n",
    "            self.frequent_items.update(frequent_candidates)\n",
    "            # replacing the l_n itemsets with the l_n+1 itemsets\n",
    "            candidates = [itemset for itemset in frequent_candidates]\n",
    "        \n",
    "    def _calculate_support(self, items):\n",
    "        records = self.mapped_records\n",
    "        return sum(1 if all(item in record for item in items) else 0 for record in records)\n",
    "    \n",
    "    def _generate(self, items):\n",
    "        new_items = []\n",
    "\n",
    "        # generating\n",
    "        for i in range(len(items) - 1):\n",
    "            for k in range(i + 1, len(items)):\n",
    "                if items[i][:-1] == items[k][:-1]:\n",
    "                    new_items.append(tuple([item for item in items[i]] + [items[k][-1]]))\n",
    "                                        \n",
    "        pruned_new_items = []\n",
    "        # pruning\n",
    "        for item in new_items:\n",
    "            if all((tuple(item[:i] + item[i + 1:]) in items) for i in range(len(item))):\n",
    "                pruned_new_items.append(item)\n",
    "\n",
    "        return pruned_new_items\n",
    "    \n",
    "    def recommend(self, items):\n",
    "        item_ids = sorted(self.item_to_id[item] for item in items)\n",
    "\n",
    "        # Filtering the frequent (n+1)-itemsets that contain the items we need a recommendation to.\n",
    "        # We do not need to consider (>n+1)-itemsets as these contain only the items from the (n+1)-itemsets.\n",
    "        # In this case I recommend another item only if it is in a frequent dataset.\n",
    "        # In some context it might be the desired behaviour.\n",
    "        frequent_itemsets_query = [record for record in self.frequent_items\n",
    "                                              if len(record) == len(item_ids)+1 and all(item_id in record for item_id in item_ids)]\n",
    "\n",
    "        # finding the maximum support of the (n+1)-itemsets\n",
    "        supports = tuple(self.frequent_items[itemset] for itemset in frequent_itemsets_query)\n",
    "        if len(supports) == 0:\n",
    "            print(\"I cannot recommend any item with a\")\n",
    "            return [], 0\n",
    "        max_support = max(supports)\n",
    "\n",
    "        # filtering the (n+1)-itemsets with the maximum support\n",
    "        recommendations = tuple(filter(lambda itemset: self.frequent_items[itemset] == max_support,\n",
    "                                       frequent_itemsets_query))\n",
    "\n",
    "        # recommending every item with the maximum support\n",
    "        items = []\n",
    "        for recommendation in recommendations:\n",
    "                items.append([self.id_to_item[item_id] for item_id in recommendation \n",
    "                                  if item_id not in item_ids])\n",
    "        \n",
    "        confidence = self.frequent_items[recommendation] / self.frequent_items[tuple(item_ids)]\n",
    "        return items, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848b1c28-5159-4c5e-9a75-7e7ae11b5fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['B'], ['C']], 0.6666666666666666)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_records = [[\"A\", \"B\", \"C\"], [\"A\", \"B\"], [\"A\", \"C\"]]\n",
    "apriori = Apriori(input_records, 0.4)\n",
    "apriori.recommend([\"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35612990-2794-4436-ba78-b44602978a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['B']], 0.75)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_records = [[\"A\", \"B\", \"C\"], [\"A\", \"B\"], [\"A\", \"C\"], [\"A\", \"B\"]]\n",
    "apriori = Apriori(input_records, 0)\n",
    "apriori.recommend([\"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13041461-a8d0-4070-bde7-abdff0561718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot recommend any item with a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_records = [[\"A\", \"B\", \"C\"], [\"C\", \"D\"], [\"B\", \"C\"]]\n",
    "apriori = Apriori(input_records, 0.4)\n",
    "apriori.recommend([\"A\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea237d-8480-471e-9e26-34aa788e8e7d",
   "metadata": {},
   "source": [
    "## Findings\n",
    "In the example below both C and D are recommended as they have the confidence. However, if we look at the data, C appears more often with A and B separately than D does.\n",
    "\n",
    "The confidence of recommending $B$ to the item(s) $A$ is defined as $\\text{support}(A \\cup B)\\,/\\,\\text{support}(A)$ which means that it only considers how many times $A \\cup B$ and $A$ appears in the data. There can be cases like the one below when there are multiple options with the same confidence, but a subsets of one of them appears more often than the subsets of the other. Depending on the context, recommending $D$ might be not as good as recommending $C$. In these cases, we need another definition that considers the subsets with the recommended item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9499bf-8752-4aa1-909c-e62740dd444f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['C'], ['D']], 0.5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_records = [[\"A\", \"B\", \"C\"], [\"A\", \"B\", \"D\"], [\"B\", \"C\"], [\"A\", \"C\"], [\"A\", \"C\"], [\"B\", \"C\"]]\n",
    "apriori = Apriori(input_records, 0)\n",
    "apriori.recommend([\"A\", \"B\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
